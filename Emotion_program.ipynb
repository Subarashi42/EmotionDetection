{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from collections import deque\n",
    "\n",
    "# Load emotion recognition model and metadata\n",
    "model = tf.keras.models.load_model('emotion_recognition_5class_final.h5')\n",
    "with open('emotion_recognition_5class_final_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Load DNN face detector\n",
    "face_net = cv2.dnn.readNetFromCaffe(\n",
    "    'deploy.prototxt',\n",
    "    'res10_300x300_ssd_iter_140000.caffemodel'\n",
    ")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_image(image):\n",
    "    image = cv2.resize(image, (48, 48))\n",
    "    if len(image.shape) == 3 and image.shape[2] > 1:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = image / 255.0\n",
    "    image = image.reshape(1, 48, 48, 1)\n",
    "    return image\n",
    "\n",
    "# Emotion colors\n",
    "emotion_colors = {\n",
    "    \"Engaged\": (46, 250, 87),     # Dark green\n",
    "    \"Frustrated\": (51, 51, 204),  # Red (BGR format)\n",
    "    \"Disengaged\": (204, 51, 51),  # Blue (BGR format)\n",
    "    \"Surprise\": (0, 215, 255),    # Yellow (BGR format)\n",
    "    \"Neutral\": (96, 96, 96)       # Gray\n",
    "}\n",
    "\n",
    "emotion_counts = {emotion: 0 for emotion in emotion_colors}\n",
    "emotion_timestamps = {emotion: [] for emotion in emotion_colors}\n",
    "start_time = time.time()\n",
    "prediction_buffer = deque(maxlen=10)\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # DNN face detection\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n",
    "                                 (104.0, 177.0, 123.0))\n",
    "    face_net.setInput(blob)\n",
    "    detections = face_net.forward()\n",
    "\n",
    "    h, w = frame.shape[:2]\n",
    "    faces = []\n",
    "\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.6:\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            x1, y1, x2, y2 = box.astype(\"int\")\n",
    "            faces.append((x1, y1, x2 - x1, y2 - y1))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_img = frame[y:y+h, x:x+w]\n",
    "        processed_img = preprocess_image(face_img)\n",
    "\n",
    "        raw_preds = model.predict(processed_img)[0]\n",
    "        prediction_buffer.append(raw_preds)\n",
    "        avg_preds = np.mean(prediction_buffer, axis=0)\n",
    "\n",
    "        pred_class = np.argmax(avg_preds)\n",
    "        emotion = metadata['emotion_map'][str(pred_class)]\n",
    "        confidence = float(avg_preds[pred_class])\n",
    "\n",
    "        current_time = datetime.datetime.now()\n",
    "        emotion_counts[emotion] += 1\n",
    "        emotion_timestamps[emotion].append(current_time)\n",
    "\n",
    "        box_color = emotion_colors.get(emotion, (255, 255, 255))\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), box_color, 2)\n",
    "\n",
    "        label = f'{emotion}'\n",
    "        cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.9, box_color, 2, lineType=cv2.LINE_AA)\n",
    "\n",
    "        bar_x = x + w + 10\n",
    "        bar_y = y\n",
    "        bar_width = 50\n",
    "        bar_height = h // len(metadata['emotion_map'])\n",
    "\n",
    "        for i, (key, name) in enumerate(metadata['emotion_map'].items()):\n",
    "            prob = avg_preds[int(key)]\n",
    "            bar_length = int(prob * bar_width)\n",
    "            color = emotion_colors.get(name, (255, 255, 255))\n",
    "            y_pos = bar_y + i * bar_height\n",
    "\n",
    "            cv2.rectangle(frame, (bar_x, y_pos),\n",
    "                (bar_x + bar_width, y_pos + bar_height - 4),\n",
    "                (0, 0, 0), -1, lineType=cv2.LINE_AA)\n",
    "            cv2.rectangle(frame, (bar_x, y_pos),\n",
    "                (bar_x + bar_width, y_pos + bar_height - 4),\n",
    "                color, 1, lineType=cv2.LINE_AA)\n",
    "            cv2.rectangle(frame, (bar_x, y_pos),\n",
    "                (bar_x + bar_length, y_pos + bar_height - 4),\n",
    "                color, -1, lineType=cv2.LINE_AA)\n",
    "            cv2.putText(frame, name, (bar_x + bar_width + 6, y_pos + bar_height - 6),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.42, (50, 50, 50), 2, lineType=cv2.LINE_AA)\n",
    "            cv2.putText(frame, name, (bar_x + bar_width + 6, y_pos + bar_height - 6),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.42, color, 1, lineType=cv2.LINE_AA)\n",
    "\n",
    "    session_time = time.time() - start_time\n",
    "    stats_y = 30\n",
    "    cv2.putText(frame, f\"Session time: {int(session_time)}s\", (10, stats_y), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, lineType=cv2.LINE_AA)\n",
    "\n",
    "    for idx, (emotion_name, count) in enumerate(emotion_counts.items()):\n",
    "        stats_y += 25\n",
    "        color = emotion_colors.get(emotion_name, (255, 255, 255))\n",
    "        cv2.putText(frame, f\"{emotion_name}: {count}\", (10, stats_y), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 1, lineType=cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Emotion Recognition', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Save session data\n",
    "data_to_save = {\n",
    "    \"total_session_time\": time.time() - start_time,\n",
    "    \"emotion_counts\": emotion_counts,\n",
    "    \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "}\n",
    "\n",
    "with open(f\"emotion_session_data_{data_to_save['timestamp']}.json\", 'w') as f:\n",
    "    json.dump(data_to_save, f, indent=4, default=str)\n",
    "\n",
    "print(\"Emotion data saved to file.\")\n",
    "cap.release()\n",
    "cv2.destroyWindow('Emotion Recognition')\n",
    "cv2.waitKey(1)\n",
    "time.sleep(0.5)\n",
    "\n",
    "# generate a report based on the saved data with graphs\n",
    "def generate_report(data_file):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.colors import rgb2hex\n",
    "    \n",
    "    with open(data_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Text report portion\n",
    "    report = f\"Emotion Recognition Session Report\\n\"\n",
    "    report += f\"Session Time: {data['total_session_time']:.2f} seconds\\n\"\n",
    "    report += f\"Timestamp: {data['timestamp']}\\n\\n\"\n",
    "    report += \"Emotion Counts:\\n\"\n",
    "    \n",
    "    total_emotions = sum(data['emotion_counts'].values())\n",
    "    \n",
    "    for emotion, count in data['emotion_counts'].items():\n",
    "        percentage = (count / total_emotions * 100) if total_emotions > 0 else 0\n",
    "        report += f\"{emotion}: {count} ({percentage:.1f}%)\\n\"\n",
    "    \n",
    "    # Create visualizations\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Pie chart\n",
    "    plt.subplot(1, 2, 1)\n",
    "    labels = list(data['emotion_counts'].keys())\n",
    "    counts = list(data['emotion_counts'].values())\n",
    "    \n",
    "    # Map emotion_colors to pie chart colors\n",
    "    colors = [rgb2hex([c[2]/255, c[1]/255, c[0]/255]) for c in [emotion_colors.get(emotion, (255, 255, 255)) for emotion in labels]]\n",
    "    \n",
    "    # Only include emotions with count > 0 in the pie chart\n",
    "    non_zero_labels = []\n",
    "    non_zero_counts = []\n",
    "    non_zero_colors = []\n",
    "    \n",
    "    for i, count in enumerate(counts):\n",
    "        if count > 0:\n",
    "            non_zero_labels.append(labels[i])\n",
    "            non_zero_counts.append(count)\n",
    "            non_zero_colors.append(colors[i])\n",
    "    \n",
    "    if sum(non_zero_counts) > 0:  # Only create pie chart if there's data\n",
    "        plt.pie(non_zero_counts, labels=non_zero_labels, colors=non_zero_colors, \n",
    "                autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "        plt.axis('equal')\n",
    "        plt.title('Emotion Distribution')\n",
    "    \n",
    "    # Bar chart\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(labels, counts, color=colors)\n",
    "    plt.title('Emotion Counts')\n",
    "    plt.xlabel('Emotion')\n",
    "    plt.ylabel('Count')\n",
    "    for i, v in enumerate(counts):\n",
    "        plt.text(i, v + 0.5, str(v), ha='center')\n",
    "    \n",
    "    # Save the figure\n",
    "    chart_file = f\"emotion_session_chart_{data['timestamp']}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(chart_file)\n",
    "    plt.close()\n",
    "    \n",
    "    # Add chart file info to report\n",
    "    report += f\"\\nEmotional analysis charts saved to: {chart_file}\\n\"\n",
    "    \n",
    "    # Create a summary section\n",
    "    report += \"\\nSUMMARY ANALYSIS:\\n\"\n",
    "    if total_emotions > 0:\n",
    "        dominant_emotion = max(data['emotion_counts'].items(), key=lambda x: x[1])[0]\n",
    "        report += f\"- Dominant emotion: {dominant_emotion}\\n\"\n",
    "        \n",
    "        # Calculate engagement score (engaged vs disengaged ratio)\n",
    "        engaged = data['emotion_counts'].get('Engaged', 0)\n",
    "        disengaged = data['emotion_counts'].get('Disengaged', 0)\n",
    "        \n",
    "        if (engaged + disengaged) > 0:\n",
    "            engagement_ratio = engaged / (engaged + disengaged) if (engaged + disengaged) > 0 else 0\n",
    "            report += f\"- Engagement score: {engagement_ratio:.2f} ({engagement_ratio*100:.1f}%)\\n\"\n",
    "            \n",
    "            if engagement_ratio > 0.8:\n",
    "                report += \"  Interpretation: High engagement level\\n\"\n",
    "            elif engagement_ratio > 0.5:\n",
    "                report += \"  Interpretation: Moderate engagement level\\n\"\n",
    "            else:\n",
    "                report += \"  Interpretation: Low engagement level\\n\"\n",
    "    else:\n",
    "        report += \"- No emotions detected during this session\\n\"\n",
    "    \n",
    "    return report, chart_file\n",
    "\n",
    "# Generate and print the report\n",
    "report, chart_file = generate_report(f\"emotion_session_data_{data_to_save['timestamp']}.json\")\n",
    "print(\"\\nGenerated Report:\")\n",
    "print(report)\n",
    "\n",
    "# Save the report to a text file\n",
    "report_file = f\"emotion_session_report_{data_to_save['timestamp']}.txt\"\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(report)\n",
    "print(f\"Report saved to {report_file}.\")\n",
    "print(f\"Charts saved to {chart_file}.\")\n",
    "\n",
    "# Display the chart in the notebook\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=chart_file))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
